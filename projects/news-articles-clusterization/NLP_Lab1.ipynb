{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "enFbByrX5FgV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.makedirs(\"assets/annotated-corpus/train\")\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    os.makedirs(\"assets/annotated-corpus/test\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "V6pCKiBH_M_N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка nltk ресурсов\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Инициализация стеммера и лемматизатора\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIT9S9hD7kK5",
        "outputId": "06b9f229-606e-4fbe-b158-d674dc04e0a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reg(text):\n",
        "    # Регулярные выражения для различных случаев\n",
        "  phone_pattern = r'\\+?[78]?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{2}[-.\\s]?\\d{2}'\n",
        "  address_pattern = r'\\b(?:blv\\.|st\\.|ave\\.|app\\.)[\\s.,]*\\S+'\n",
        "  email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "  emoji_pattern = r'[\\w\\s]+(?::|;|X|\\^)(?:-)?(?:\\)|\\(|D|P)'\n",
        "  math_pattern = r'[a-zA-Z0-9\\*\\+\\-\\=\\/\\(\\)\\[\\]\\{\\}\\^\\%\\$\\@\\#\\~\\,]+'\n",
        "  abbreviation_pattern = r'\\b[A-Z][a-z]*\\.'\n",
        "\n",
        "  # Комбинированное регулярное выражение для токенизации\n",
        "  combined_pattern = \"|\".join((phone_pattern, address_pattern, email_pattern, emoji_pattern, math_pattern, abbreviation_pattern, \"\\w]+\"))\n",
        "  tokens = re.findall(combined_pattern, text)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "Jh98hDSz89T3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"+79112451944 aaaaa ave. Street 17 my@perfect.email aaaaaaa :D X) (943%) USA GmBH\"\n",
        "reg(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbjIxA0tDRzb",
        "outputId": "7c17c237-d0aa-4368-eff3-223323f831ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['+79112451944',\n",
              " 'aaaaa',\n",
              " 'ave. Street',\n",
              " '17',\n",
              " 'my@perfect.email',\n",
              " ' aaaaaaa :D',\n",
              " ' X)',\n",
              " '(943%)',\n",
              " 'USA',\n",
              " 'GmBH']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EyGgco0Z37A6"
      },
      "outputs": [],
      "source": [
        "def stem(data, subset):\n",
        "    all_sentences = data.values.tolist()\n",
        "\n",
        "    for i in range(len(all_sentences)):\n",
        "        el = all_sentences[i]\n",
        "        text = el[1] + \" \" + el[2]\n",
        "        label = el[0]\n",
        "\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z0-9])', text)\n",
        "        for sentence in sentences:\n",
        "            tokens = reg(sentence)\n",
        "            for token in tokens:\n",
        "                with open(f'assets/annotated-corpus/{subset}/{label}.tsv', 'a', newline='') as tsvfile:\n",
        "                    writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
        "                    writer.writerow([i, token, stemmer.stem(token), lemmatizer.lemmatize(token)])\n",
        "            with open(f'assets/annotated-corpus/{subset}/{label}.tsv', 'a', newline='') as tsvfile:\n",
        "                writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
        "                writer.writerow([i, \"<endofsentence>\", \"<endofsentence>\", \"<endofsentence>\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem(pd.read_csv(\"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\", header=None), \"train\")"
      ],
      "metadata": {
        "id": "3QIs3mf67VNE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem(pd.read_csv(\"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\", header=None), \"test\")"
      ],
      "metadata": {
        "id": "cl9DDz8a7WBJ"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}